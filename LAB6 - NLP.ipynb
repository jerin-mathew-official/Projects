{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df42272c",
   "metadata": {},
   "source": [
    "# Lab Program 6 - NLP \n",
    "\n",
    "# NGRAM SEQUENCING & POS TAGGING\n",
    "\n",
    "## Name: Jerin Mathew\n",
    "\n",
    "## Roll No: 2139455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f995cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec46f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('C:/ClassesMSC/DataSets/realdonaldtrump.csv')\n",
    "col = df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d121776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'Don', '’', 't', 'be', 'afraid', 'of', 'being', 'unique', '-', 'it', \"'s\", 'like', 'being', 'afraid', 'of', 'your', 'best', 'self', '.', \"''\", '--', 'Donald', 'J.', 'Trump', 'http', ':', '//tinyurl.com/pqpfvm']\n"
     ]
    }
   ],
   "source": [
    "word_tok = nltk.word_tokenize(col.iloc[10])\n",
    "print(word_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61beb09b",
   "metadata": {},
   "source": [
    "# N - Gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f61f0888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('``', 'Don', '’', 't')\n",
      "('Don', '’', 't', 'be')\n",
      "('’', 't', 'be', 'afraid')\n",
      "('t', 'be', 'afraid', 'of')\n",
      "('be', 'afraid', 'of', 'being')\n",
      "('afraid', 'of', 'being', 'unique')\n",
      "('of', 'being', 'unique', '-')\n",
      "('being', 'unique', '-', 'it')\n",
      "('unique', '-', 'it', \"'s\")\n",
      "('-', 'it', \"'s\", 'like')\n",
      "('it', \"'s\", 'like', 'being')\n",
      "(\"'s\", 'like', 'being', 'afraid')\n",
      "('like', 'being', 'afraid', 'of')\n",
      "('being', 'afraid', 'of', 'your')\n",
      "('afraid', 'of', 'your', 'best')\n",
      "('of', 'your', 'best', 'self')\n",
      "('your', 'best', 'self', '.')\n",
      "('best', 'self', '.', \"''\")\n",
      "('self', '.', \"''\", '--')\n",
      "('.', \"''\", '--', 'Donald')\n",
      "(\"''\", '--', 'Donald', 'J.')\n",
      "('--', 'Donald', 'J.', 'Trump')\n",
      "('Donald', 'J.', 'Trump', 'http')\n",
      "('J.', 'Trump', 'http', ':')\n",
      "('Trump', 'http', ':', '//tinyurl.com/pqpfvm')\n"
     ]
    }
   ],
   "source": [
    "NGRAMS=ngrams(sequence=nltk.word_tokenize(col.iloc[10]), n=4)\n",
    "for grams in NGRAMS:\n",
    " print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ecf0558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('``', 'Don', '’')\n",
      "('Don', '’', 't')\n",
      "('’', 't', 'be')\n",
      "('t', 'be', 'afraid')\n",
      "('be', 'afraid', 'of')\n",
      "('afraid', 'of', 'being')\n",
      "('of', 'being', 'unique')\n",
      "('being', 'unique', '-')\n",
      "('unique', '-', 'it')\n",
      "('-', 'it', \"'s\")\n",
      "('it', \"'s\", 'like')\n",
      "(\"'s\", 'like', 'being')\n",
      "('like', 'being', 'afraid')\n",
      "('being', 'afraid', 'of')\n",
      "('afraid', 'of', 'your')\n",
      "('of', 'your', 'best')\n",
      "('your', 'best', 'self')\n",
      "('best', 'self', '.')\n",
      "('self', '.', \"''\")\n",
      "('.', \"''\", '--')\n",
      "(\"''\", '--', 'Donald')\n",
      "('--', 'Donald', 'J.')\n",
      "('Donald', 'J.', 'Trump')\n",
      "('J.', 'Trump', 'http')\n",
      "('Trump', 'http', ':')\n",
      "('http', ':', '//tinyurl.com/pqpfvm')\n"
     ]
    }
   ],
   "source": [
    "NGRAMS=ngrams(sequence=nltk.word_tokenize(col.iloc[10]), n=3)\n",
    "for grams in NGRAMS:\n",
    " print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bcca03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('``', 'Don')\n",
      "('Don', '’')\n",
      "('’', 't')\n",
      "('t', 'be')\n",
      "('be', 'afraid')\n",
      "('afraid', 'of')\n",
      "('of', 'being')\n",
      "('being', 'unique')\n",
      "('unique', '-')\n",
      "('-', 'it')\n",
      "('it', \"'s\")\n",
      "(\"'s\", 'like')\n",
      "('like', 'being')\n",
      "('being', 'afraid')\n",
      "('afraid', 'of')\n",
      "('of', 'your')\n",
      "('your', 'best')\n",
      "('best', 'self')\n",
      "('self', '.')\n",
      "('.', \"''\")\n",
      "(\"''\", '--')\n",
      "('--', 'Donald')\n",
      "('Donald', 'J.')\n",
      "('J.', 'Trump')\n",
      "('Trump', 'http')\n",
      "('http', ':')\n",
      "(':', '//tinyurl.com/pqpfvm')\n"
     ]
    }
   ],
   "source": [
    "NGRAMS=ngrams(sequence=nltk.word_tokenize(col.iloc[10]), n=2)\n",
    "for grams in NGRAMS:\n",
    " print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "589ade5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('``',)\n",
      "('Don',)\n",
      "('’',)\n",
      "('t',)\n",
      "('be',)\n",
      "('afraid',)\n",
      "('of',)\n",
      "('being',)\n",
      "('unique',)\n",
      "('-',)\n",
      "('it',)\n",
      "(\"'s\",)\n",
      "('like',)\n",
      "('being',)\n",
      "('afraid',)\n",
      "('of',)\n",
      "('your',)\n",
      "('best',)\n",
      "('self',)\n",
      "('.',)\n",
      "(\"''\",)\n",
      "('--',)\n",
      "('Donald',)\n",
      "('J.',)\n",
      "('Trump',)\n",
      "('http',)\n",
      "(':',)\n",
      "('//tinyurl.com/pqpfvm',)\n"
     ]
    }
   ],
   "source": [
    "NGRAMS=ngrams(sequence=nltk.word_tokenize(col.iloc[10]), n=1)\n",
    "for grams in NGRAMS:\n",
    " print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44364c7",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81bed87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'Don', '’', 't', 'be', 'afraid', 'of', 'being', 'unique', '-', 'it', \"'s\", 'like', 'being', 'afraid', 'of', 'your', 'best', 'self', '.', \"''\", '--', 'Donald', 'J.', 'Trump', 'http', ':', '//tinyurl.com/pqpfvm']\n"
     ]
    }
   ],
   "source": [
    "word_tok = nltk.word_tokenize(col.iloc[10])\n",
    "print(word_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acd9770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tok = [w for w in word_tok if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e16c9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('``', '``'), ('Don', 'NNP'), ('’', 'NNP'), ('afraid', 'VBD'), ('unique', 'JJ'), ('-', ':'), (\"'s\", 'POS'), ('like', 'IN'), ('afraid', 'JJ'), ('best', 'JJS'), ('self', 'NN'), ('.', '.'), (\"''\", \"''\"), ('--', ':'), ('Donald', 'NNP'), ('J.', 'NNP'), ('Trump', 'NNP'), ('http', 'NN'), (':', ':'), ('//tinyurl.com/pqpfvm', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(word_tok)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214038ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
